{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from common import Paper\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "sns.set()\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_papers():\n",
    "    concat = []\n",
    "\n",
    "    for f in tqdm(glob.glob('./papers/*.pkl')):\n",
    "        _, field, year, month = os.path.basename(f).split('.')[0].split('-')\n",
    "        raw = pickle.load(open(f, 'rb'))\n",
    "        concat += [x.__dict__ for x in raw]\n",
    "\n",
    "    return pd.DataFrame(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words = stop_words.union([\"using\", \"show\", \"result\", \"large\", \n",
    "                               \"also\", \"iv\", \"one\", \"two\", \"new\", \n",
    "                               \"previously\", \"shown\", \"cite\", \"work\", \"other\"\n",
    "                               \"however\", \"thus\", \"therefore\", \"while\", \"whilst\", \"continues\"])\n",
    "\n",
    "papers = get_papers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Clean dataset and construct corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure $\\geq$ N words in each abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WORDS = 20\n",
    "\n",
    "papers['word_count'] = papers.loc[:, 'abstract'].apply(lambda x: len(str(x).split(\" \")))\n",
    "papers = papers[papers['word_count'] >= MIN_WORDS]\n",
    "\n",
    "papers.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon = pd.Series(' '.join(papers['abstract']).split()).value_counts()[-20:]\n",
    "# uncommon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_corpus(dataset):\n",
    "    corpus_map = {}\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        d = dataset.iloc[i]\n",
    "        \n",
    "        text = f'{d.title} {d.abstract}'\n",
    "        text = re.sub('[^a-zA-Z-]', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(\"&lt;/?.*?&gt;\", \" &lt;&gt; \",text)\n",
    "\n",
    "        text = ' '.join([wnl.lemmatize(w) for w in text.split() if not w in stop_words])\n",
    "        \n",
    "        corpus_map[d.pdf] = text\n",
    "        \n",
    "    return list(corpus_map.values()), corpus_map\n",
    "\n",
    "corpus, corpus_map = construct_corpus(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# wordcloud = WordCloud(background_color='white',\n",
    "#                       stopwords=stop_words,\n",
    "#                       max_words=100,\n",
    "#                       max_font_size=50).generate(str(corpus))\n",
    "\n",
    "# fig = plt.figure(figsize=(3, 2), dpi=250)\n",
    "# plt.imshow(wordcloud, interpolation='lanczos')\n",
    "# plt.axis('off')\n",
    "# pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cv = CountVectorizer(max_df=0.8, stop_words=stop_words, max_features=10_000, ngram_range=(1,2))\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "tf_idf = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tf_idf.fit(X)\n",
    "\n",
    "features = cv.get_feature_names()\n",
    "\n",
    "pickle.dump([cv, tf_idf, features], open('vec/cv_tfidf_feat.pkl', 'wb'))\n",
    "pickle.dump(corpus_map, open('vec/corpus_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(doc, n=5, only_words=False):\n",
    "    tf_idf_vec = tf_idf.transform(cv.transform([doc]))\n",
    "    \n",
    "    coo = tf_idf_vec.tocoo()\n",
    "    sorted_items = sorted(zip(coo.col, coo.data), key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    \n",
    "    if only_words:\n",
    "        return [features[idx] for (idx, _) in sorted_items[:n]]\n",
    "    \n",
    "    return {features[idx]: round(score, 3) for (idx, score) in sorted_items[:n]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = random.choice(list(corpus_map.keys()))\n",
    "keywords = get_keywords(corpus_map[k], n=10)\n",
    "\n",
    "p = papers[papers.pdf == k]\n",
    "\n",
    "print(\"\\nTitle:\")\n",
    "print(str(p.title.values[0]))\n",
    "print(\"\\nAbstract:\")\n",
    "print(str(p.abstract.values[0]))\n",
    "print(\"\\nKeywords:\")\n",
    "for kw, score in keywords.items():\n",
    "    print(f'{kw:>32s}: {score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
