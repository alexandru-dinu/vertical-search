{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from common import Paper\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "sns.set()\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_papers():\n",
    "    concat = []\n",
    "\n",
    "    for f in tqdm(glob.glob('./papers/*.pkl')):\n",
    "        _, field, year, month = os.path.basename(f).split('.')[0].split('-')\n",
    "        raw = pickle.load(open(f, 'rb'))\n",
    "        concat += [x.__dict__ for x in raw]\n",
    "\n",
    "    return pd.DataFrame(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 540/540 [00:00<00:00, 807.75it/s]\n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words = stop_words.union([\"using\", \"show\", \"result\", \"large\", \n",
    "                               \"also\", \"iv\", \"one\", \"two\", \"new\", \n",
    "                               \"previously\", \"shown\", \"cite\", \"work\", \"other\"\n",
    "                               \"however\", \"thus\", \"therefore\", \"while\", \"whilst\", \"continues\"])\n",
    "\n",
    "papers = get_papers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Clean dataset and construct corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure $\\geq$ N words in each abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    110909.000000\n",
       "mean        158.822999\n",
       "std          50.537725\n",
       "min          20.000000\n",
       "25%         123.000000\n",
       "50%         155.000000\n",
       "75%         191.000000\n",
       "max         559.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIN_WORDS = 20\n",
    "\n",
    "papers['word_count'] = papers.loc[:, 'abstract'].apply(lambda x: len(str(x).split(\" \")))\n",
    "papers = papers[papers['word_count'] >= MIN_WORDS]\n",
    "\n",
    "papers.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon = pd.Series(' '.join(papers['abstract']).split()).value_counts()[-20:]\n",
    "# uncommon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110909/110909 [00:57<00:00, 1937.43it/s]\n"
     ]
    }
   ],
   "source": [
    "def construct_corpus(dataset):\n",
    "    corpus_map = {}\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        d = dataset.iloc[i]\n",
    "        \n",
    "        text = f'{d.title} {d.abstract}'\n",
    "        text = re.sub('[^a-zA-Z-]', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(\"&lt;/?.*?&gt;\", \" &lt;&gt; \",text)\n",
    "\n",
    "        text = ' '.join([wnl.lemmatize(w) for w in text.split() if not w in stop_words])\n",
    "        \n",
    "        corpus_map[d.pdf] = text\n",
    "        \n",
    "    return list(corpus_map.values()), corpus_map\n",
    "\n",
    "corpus, corpus_map = construct_corpus(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# wordcloud = WordCloud(background_color='white',\n",
    "#                       stopwords=stop_words,\n",
    "#                       max_words=100,\n",
    "#                       max_font_size=50).generate(str(corpus))\n",
    "\n",
    "# fig = plt.figure(figsize=(3, 2), dpi=250)\n",
    "# plt.imshow(wordcloud, interpolation='lanczos')\n",
    "# plt.axis('off')\n",
    "# pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.3 s, sys: 608 ms, total: 23.9 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = CountVectorizer(max_df=0.8, stop_words=stop_words, max_features=10_000, ngram_range=(1,2))\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "tf_idf = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tf_idf.fit(X)\n",
    "\n",
    "features = cv.get_feature_names()\n",
    "\n",
    "pickle.dump([cv, tf_idf, features], open('vec/cv_tfidf_feat.pkl', 'wb'))\n",
    "pickle.dump(corpus_map, open('vec/corpus_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(doc, n=5, only_words=False):\n",
    "    tf_idf_vec = tf_idf.transform(cv.transform([doc]))\n",
    "    \n",
    "    coo = tf_idf_vec.tocoo()\n",
    "    sorted_items = sorted(zip(coo.col, coo.data), key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    \n",
    "    if only_words:\n",
    "        return [features[idx] for (idx, _) in sorted_items[:n]]\n",
    "    \n",
    "    return {features[idx]: round(score, 3) for (idx, score) in sorted_items[:n]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title:\n",
      "An Overview of Hierarchical Task Network Planning\n",
      "\n",
      "Abstract:\n",
      "Hierarchies are the most common structure used to understand the world better. In galaxies, for instance, multiple-star systems are organised in a hierarchical system. Then, governmental and company organisations are structured using a hierarchy, while the Internet, which is used on a daily basis, has a space of domain names arranged hierarchically. Since Artificial Intelligence (AI) planning portrays information about the world and reasons to solve some of world\\'s problems, Hierarchical Task Network (HTN) planning has been introduced almost 40 years ago to represent and deal with hierarchies. Its requirement for rich domain knowledge to characterise the world enables HTN planning to be very useful, but also to perform well. However, the history of almost 40 years obfuscates the current understanding of HTN planning in terms of accomplishments, planning models, similarities and differences among hierarchical planners, and its current and objective image. On top of these issues, attention attracts the ability of hierarchical planning to truly cope with the requirements of applications from the real world. We propose a framework-based approach to remedy this situation. First, we provide a basis for defining different formal models of hierarchical planning, and define two models that comprise a large portion of HTN planners. Second, we provide a set of concepts that helps to interpret HTN planners from the aspect of their search space. Then, we analyse and compare the planners based on a variety of properties organised in five segments, namely domain authoring, expressiveness, competence, performance and applicability. Furthermore, we select Web service composition as a real-world and current application, and classify and compare the approaches that employ HTN planning to solve the problem of service composition. Finally, we conclude with our findings and present directions for future work. \n",
      "\n",
      "Keywords:\n",
      "                        planning: 0.497\n",
      "                    hierarchical: 0.302\n",
      "                         planner: 0.295\n",
      "                           world: 0.223\n",
      "                       hierarchy: 0.189\n",
      "                    task network: 0.167\n",
      "                     composition: 0.13\n",
      "                         current: 0.124\n",
      "                          almost: 0.115\n",
      "                         service: 0.115\n"
     ]
    }
   ],
   "source": [
    "k = random.choice(list(corpus_map.keys()))\n",
    "keywords = get_keywords(corpus_map[k], n=10)\n",
    "\n",
    "p = papers[papers.pdf == k]\n",
    "\n",
    "print(\"\\nTitle:\")\n",
    "print(str(p.title.values[0]))\n",
    "print(\"\\nAbstract:\")\n",
    "print(str(p.abstract.values[0]))\n",
    "print(\"\\nKeywords:\")\n",
    "for kw, score in keywords.items():\n",
    "    print(f'{kw:>32s}: {score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
